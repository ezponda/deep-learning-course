{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Convolution and CNNs\n",
    "\n",
    "Understanding how convolutional neural networks see images.\n",
    "\n",
    "This notebook covers the **theory** behind CNNs. You'll apply these concepts in [09_lab_convolutional_networks](09_lab_convolutional_networks.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why CNNs for Images?\n",
    "\n",
    "A regular neural network treats each pixel as an independent input.\n",
    "\n",
    "**Problems:**\n",
    "- A 224×224 RGB image = 150,528 inputs\n",
    "- Ignores spatial structure (nearby pixels are related)\n",
    "- Not translation invariant (cat in corner ≠ cat in center)\n",
    "\n",
    "**CNNs solve this** by:\n",
    "1. Using small **kernels** that slide across the image\n",
    "2. Sharing weights across all positions\n",
    "3. Building hierarchical features (edges → shapes → objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is Convolution?\n",
    "\n",
    "Convolution is a mathematical operation that slides a small **kernel** (filter) across an image.\n",
    "\n",
    "```\n",
    "     INPUT IMAGE              KERNEL (3×3)           OUTPUT\n",
    "     \n",
    "    ┌─────────────┐          ┌───────┐\n",
    "    │ 1  2  3  0  │          │ 1 0 1 │\n",
    "    │ 0  1  2  3  │    *     │ 0 1 0 │    =    Feature Map\n",
    "    │ 3  0  1  2  │          │ 1 0 1 │\n",
    "    │ 2  3  0  1  │          └───────┘\n",
    "    └─────────────┘\n",
    "```\n",
    "\n",
    "At each position: **element-wise multiply, then sum**.\n",
    "\n",
    "**Key insight:** In CNNs, the network **learns** the best kernels for the task. We don't hand-design them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Kernels and Their Effects\n",
    "\n",
    "Different kernels detect different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "# Create a simple test image\n",
    "image = np.zeros((100, 100))\n",
    "image[30:70, 30:70] = 1  # White square\n",
    "image[40:60, 40:60] = 0.5  # Gray inner square\n",
    "\n",
    "# Define kernels\n",
    "kernels = {\n",
    "    'Original': None,\n",
    "    'Edge (Horizontal)': np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]]),\n",
    "    'Edge (Vertical)': np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]),\n",
    "    'Blur': np.ones((3, 3)) / 9,\n",
    "    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "for ax, (name, kernel) in zip(axes, kernels.items()):\n",
    "    if kernel is None:\n",
    "        ax.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        filtered = ndimage.convolve(image, kernel)\n",
    "        ax.imshow(filtered, cmap='gray')\n",
    "    ax.set_title(name)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Padding and Stride\n",
    "\n",
    "### Padding\n",
    "\n",
    "Without padding, the output is smaller than the input.\n",
    "\n",
    "```\n",
    "Input: 5×5          Kernel: 3×3          Output: 3×3 (smaller!)\n",
    "```\n",
    "\n",
    "**\"Same\" padding** adds zeros around the border to keep the same size.\n",
    "\n",
    "### Stride\n",
    "\n",
    "**Stride** controls how far the kernel moves at each step.\n",
    "\n",
    "- Stride 1: Move 1 pixel at a time (default)\n",
    "- Stride 2: Move 2 pixels at a time (reduces output size by half)\n",
    "\n",
    "```\n",
    "Stride = 1                    Stride = 2\n",
    "───────────                   ───────────\n",
    "Step 1: [x x x] _ _          Step 1: [x x x] _ _\n",
    "Step 2: _ [x x x] _          Step 2: _ _ [x x x]\n",
    "Step 3: _ _ [x x x]          (done - only 2 steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pooling\n",
    "\n",
    "Pooling **downsamples** the feature maps, reducing computation and adding translation invariance.\n",
    "\n",
    "### Max Pooling\n",
    "\n",
    "Take the maximum value in each region.\n",
    "\n",
    "```\n",
    "Input (4×4)              Max Pool 2×2              Output (2×2)\n",
    "\n",
    "┌───┬───┬───┬───┐                                 ┌───┬───┐\n",
    "│ 1 │ 3 │ 2 │ 1 │                                 │ 4 │ 6 │\n",
    "├───┼───┼───┼───┤        ──────────▶              ├───┼───┤\n",
    "│ 4 │ 2 │ 6 │ 4 │         (take max)              │ 8 │ 7 │\n",
    "├───┼───┼───┼───┤                                 └───┴───┘\n",
    "│ 1 │ 8 │ 3 │ 7 │\n",
    "├───┼───┼───┼───┤\n",
    "│ 5 │ 3 │ 2 │ 1 │\n",
    "└───┴───┴───┴───┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CNN Architecture\n",
    "\n",
    "A typical CNN has this structure:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────────────────┐\n",
    "│                           CNN ARCHITECTURE                               │\n",
    "│                                                                          │\n",
    "│   Image ──▶ [Conv+ReLU] ──▶ [Pool] ──▶ [Conv+ReLU] ──▶ [Pool] ──▶ ...   │\n",
    "│                                                                          │\n",
    "│             └─────────── FEATURE EXTRACTION ───────────┘                 │\n",
    "│                                                                          │\n",
    "│   ... ──▶ [Flatten] ──▶ [Dense] ──▶ [Dense] ──▶ [Softmax] ──▶ Classes   │\n",
    "│                                                                          │\n",
    "│           └─────────────── CLASSIFICATION ──────────────┘                │\n",
    "└──────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Typical pattern:**\n",
    "1. **Conv → ReLU → Pool** (repeat several times)\n",
    "2. **Flatten** (convert 2D to 1D)\n",
    "3. **Dense layers** (standard neural network)\n",
    "4. **Softmax** (output probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Hierarchy\n",
    "\n",
    "As we go deeper, the network learns increasingly complex features:\n",
    "\n",
    "```\n",
    "Layer 1         Layer 2         Layer 3         Layer 4\n",
    "─────────       ─────────       ─────────       ─────────\n",
    "  Edges    →    Textures   →    Parts      →    Objects\n",
    "  Lines         Patterns        Eyes, ears      Faces\n",
    "  Corners       Gradients       Wheels          Cars\n",
    "```\n",
    "\n",
    "This hierarchical feature learning is why CNNs are so powerful for vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Parameters\n",
    "\n",
    "| Parameter | Description | Typical values |\n",
    "|-----------|-------------|----------------|\n",
    "| **Filters** | Number of kernels (output channels) | 32, 64, 128 |\n",
    "| **Kernel size** | Size of the convolution window | 3×3, 5×5 |\n",
    "| **Stride** | Step size for sliding the kernel | 1, 2 |\n",
    "| **Padding** | How to handle borders | 'same', 'valid' |\n",
    "| **Pool size** | Size of pooling window | 2×2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Transfer Learning (Preview)\n",
    "\n",
    "Training CNNs from scratch requires:\n",
    "- Millions of images\n",
    "- Days of training\n",
    "- Expensive hardware\n",
    "\n",
    "**Transfer learning** solves this by reusing a model trained on a large dataset (like ImageNet).\n",
    "\n",
    "```\n",
    "PRETRAINED MODEL (e.g., MobileNet trained on ImageNet)\n",
    "───────────────────────────────────────────────────────\n",
    "\n",
    "[Conv layers]  →  [Dense]  →  [1000 classes]\n",
    "   (frozen)       (remove)     (remove)\n",
    "       ↓\n",
    "YOUR MODEL\n",
    "──────────\n",
    "\n",
    "[Conv layers]  →  [Your Dense]  →  [Your classes]\n",
    "   (frozen)        (trainable)      (e.g., 5 flowers)\n",
    "```\n",
    "\n",
    "This is covered in the CNN Lab notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Convolution** slides a kernel across an image, detecting local patterns\n",
    "2. **Kernels** are learned (not hand-designed) to detect useful features\n",
    "3. **Pooling** reduces spatial size and adds translation invariance\n",
    "4. **CNNs learn hierarchically:** edges → textures → parts → objects\n",
    "5. **Transfer learning** lets you use pretrained models on small datasets\n",
    "\n",
    "**Next:** Build CNNs in [09_lab_convolutional_networks](09_lab_convolutional_networks.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

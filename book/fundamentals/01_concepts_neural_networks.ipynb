{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Neural Networks\n",
    "\n",
    "Understanding the building blocks of deep learning.\n",
    "\n",
    "This notebook covers the **theory** behind neural networks. No framework code—just intuition and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "A neural network is a **function** that maps inputs to outputs by learning patterns from data.\n",
    "\n",
    "```\n",
    "    INPUT              HIDDEN LAYERS              OUTPUT\n",
    "    \n",
    "    x₁ ─────┐     ┌─────○─────┐     ┌─────○\n",
    "            │     │           │     │      \n",
    "    x₂ ─────┼─────┼─────○─────┼─────┼─────○───▶ ŷ\n",
    "            │     │           │     │      \n",
    "    x₃ ─────┘     └─────○─────┘     └─────○\n",
    "    \n",
    "           weights (W)    activation    weights (W)\n",
    "```\n",
    "\n",
    "**The key insight:** By adjusting the **weights**, the network can learn to approximate any function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Neuron (Perceptron)\n",
    "\n",
    "A single neuron performs two operations:\n",
    "\n",
    "**1. Linear combination:**\n",
    "\n",
    "$$z = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "**2. Activation:**\n",
    "\n",
    "$$a = \\sigma(z)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = input vector\n",
    "- $\\mathbf{w}$ = weights (learned)\n",
    "- $b$ = bias (learned)\n",
    "- $\\sigma$ = activation function\n",
    "- $a$ = output (activation)\n",
    "\n",
    "```\n",
    "        ┌─────────────────────────────────┐\n",
    "   x₁ ──┤ w₁                              │\n",
    "        │     ╲                           │\n",
    "   x₂ ──┤ w₂ ──→  Σ + b  ──→  σ(z)  ──→  │──→ output\n",
    "        │     ╱                           │\n",
    "   x₃ ──┤ w₃                              │\n",
    "        └─────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "Without them, a deep network would just be a linear function (no matter how many layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(14, 3))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(z, 1 / (1 + np.exp(-z)), 'b-', linewidth=2)\n",
    "axes[0].set_title('Sigmoid\\n$\\sigma(z) = 1/(1+e^{-z})$')\n",
    "axes[0].set_xlabel('z')\n",
    "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "axes[1].plot(z, np.tanh(z), 'g-', linewidth=2)\n",
    "axes[1].set_title('Tanh\\n$\\\\tanh(z)$')\n",
    "axes[1].set_xlabel('z')\n",
    "axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# ReLU\n",
    "axes[2].plot(z, np.maximum(0, z), 'r-', linewidth=2)\n",
    "axes[2].set_title('ReLU\\n$\\max(0, z)$')\n",
    "axes[2].set_xlabel('z')\n",
    "axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Linear\n",
    "axes[3].plot(z, z, 'm-', linewidth=2)\n",
    "axes[3].set_title('Linear\\n$z$')\n",
    "axes[3].set_xlabel('z')\n",
    "axes[3].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Each Activation\n",
    "\n",
    "| Function | Range | Use Case |\n",
    "|----------|-------|----------|\n",
    "| **Sigmoid** | (0, 1) | Binary classification output |\n",
    "| **Tanh** | (-1, 1) | Hidden layers (older networks) |\n",
    "| **ReLU** | [0, ∞) | Hidden layers (modern default) |\n",
    "| **Softmax** | (0, 1), sums to 1 | Multi-class classification output |\n",
    "| **Linear** | (-∞, ∞) | Regression output |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "The **loss function** measures how wrong the model's predictions are. Training minimizes this loss.\n",
    "\n",
    "### Regression: Mean Squared Error (MSE)\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "Penalizes large errors heavily (squared term).\n",
    "\n",
    "### Binary Classification: Binary Cross-Entropy\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
    "\n",
    "Used when output is a probability (0 to 1).\n",
    "\n",
    "### Multi-class Classification: Categorical Cross-Entropy\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{i=1}^{K} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Used with softmax output (probability distribution over classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Which Loss to Use?\n",
    "\n",
    "| Problem | Output Activation | Loss Function |\n",
    "|---------|-------------------|---------------|\n",
    "| Regression | Linear | MSE |\n",
    "| Binary classification | Sigmoid | Binary Cross-Entropy |\n",
    "| Multi-class (one-hot) | Softmax | Categorical Cross-Entropy |\n",
    "| Multi-class (integers) | Softmax | Sparse Categorical Cross-Entropy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How Networks Learn: Gradient Descent\n",
    "\n",
    "Training a neural network means finding weights that **minimize the loss**.\n",
    "\n",
    "**Gradient descent** does this by:\n",
    "1. Compute the loss for current weights\n",
    "2. Compute the **gradient** (direction of steepest increase)\n",
    "3. Update weights in the **opposite direction**\n",
    "\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w}$$\n",
    "\n",
    "Where $\\eta$ is the **learning rate** (step size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent\n",
    "def loss_fn(w):\n",
    "    return (w - 3) ** 2 + 1  # Minimum at w=3\n",
    "\n",
    "# Simulate gradient descent\n",
    "w = 0.0\n",
    "lr = 0.1\n",
    "history = [w]\n",
    "for _ in range(15):\n",
    "    gradient = 2 * (w - 3)\n",
    "    w = w - lr * gradient\n",
    "    history.append(w)\n",
    "\n",
    "# Plot\n",
    "w_range = np.linspace(-1, 6, 100)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(w_range, loss_fn(w_range), 'b-', linewidth=2, label='Loss function')\n",
    "plt.scatter(history, [loss_fn(h) for h in history], c='red', s=80, zorder=5, label='Gradient descent steps')\n",
    "plt.plot(history, [loss_fn(h) for h in history], 'r--', alpha=0.5)\n",
    "plt.xlabel('Weight (w)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Gradient Descent: Finding the Minimum', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Backpropagation (Intuition)\n",
    "\n",
    "**Backpropagation** computes gradients efficiently using the chain rule.\n",
    "\n",
    "The key insight: to know how much each weight contributes to the loss, we propagate the error **backwards** through the network.\n",
    "\n",
    "```\n",
    "FORWARD PASS (compute predictions)\n",
    "────────────────────────────────▶\n",
    "\n",
    "  Input ──▶ Layer 1 ──▶ Layer 2 ──▶ Output ──▶ Loss\n",
    "\n",
    "◀────────────────────────────────\n",
    "BACKWARD PASS (compute gradients)\n",
    "```\n",
    "\n",
    "**You don't need to implement this yourself** — frameworks like Keras handle it automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Neurons** compute weighted sums followed by an activation function\n",
    "2. **Activation functions** add non-linearity (ReLU for hidden layers, sigmoid/softmax for output)\n",
    "3. **Loss functions** measure prediction error (MSE for regression, cross-entropy for classification)\n",
    "4. **Gradient descent** minimizes the loss by iteratively updating weights\n",
    "5. **Backpropagation** efficiently computes gradients for all weights\n",
    "\n",
    "**Next:** Learn about training in [02_concepts_training](02_concepts_training.ipynb), then apply these concepts in [03_lab_first_neural_network](03_lab_first_neural_network.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

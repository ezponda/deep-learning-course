{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Optimizers & Learning Dynamics\n",
    "\n",
    "Understanding how neural networks find optimal weights.\n",
    "\n",
    "This notebook covers **optimization algorithms** — the engines that power training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Optimizers Matter\n",
    "\n",
    "Training a neural network means finding weights that minimize the loss function. **Optimizers** determine *how* we navigate the loss landscape.\n",
    "\n",
    "The wrong optimizer (or wrong settings) can lead to:\n",
    "- Training that never converges\n",
    "- Getting stuck in poor local minima\n",
    "- Painfully slow training\n",
    "- Unstable, oscillating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Gradient Descent Recap\n",
    "\n",
    "All optimizers are variations of **gradient descent**:\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\cdot \\nabla \\mathcal{L}(w_t)$$\n",
    "\n",
    "Where:\n",
    "- $w_t$ = current weights\n",
    "- $\\eta$ = learning rate\n",
    "- $\\nabla \\mathcal{L}$ = gradient of the loss\n",
    "\n",
    "The challenge: vanilla gradient descent has problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the problem: a \"ravine\" loss surface\n",
    "def loss_surface(x, y):\n",
    "    return 0.1 * x**2 + y**2  # Elongated bowl\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = loss_surface(X, Y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "contour = ax.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax.set_xlabel('Weight 1')\n",
    "ax.set_ylabel('Weight 2')\n",
    "ax.set_title('Loss Surface: The \"Ravine\" Problem\\n(Vanilla GD oscillates in the steep direction)', fontsize=12)\n",
    "ax.set_aspect('equal')\n",
    "plt.colorbar(contour, label='Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SGD: Stochastic Gradient Descent\n",
    "\n",
    "Instead of computing gradients on the entire dataset, use **mini-batches**.\n",
    "\n",
    "**Pros:**\n",
    "- Faster iterations\n",
    "- Noise helps escape local minima\n",
    "- Works with large datasets\n",
    "\n",
    "**Cons:**\n",
    "- Noisy updates\n",
    "- Sensitive to learning rate\n",
    "- Can oscillate in ravines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum\n",
    "\n",
    "Add \"velocity\" to smooth out updates:\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + \\nabla \\mathcal{L}(w_t)$$\n",
    "$$w_{t+1} = w_t - \\eta \\cdot v_t$$\n",
    "\n",
    "Where $\\beta$ (typically 0.9) controls how much history to remember.\n",
    "\n",
    "**Intuition:** Like a ball rolling downhill — it builds up speed in consistent directions and dampens oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SGD vs SGD with Momentum\n",
    "def gradient(x, y):\n",
    "    return np.array([0.2 * x, 2 * y])  # Gradient of 0.1*x^2 + y^2\n",
    "\n",
    "def simulate_sgd(start, lr, steps, momentum=0.0):\n",
    "    pos = np.array(start, dtype=float)\n",
    "    velocity = np.zeros(2)\n",
    "    history = [pos.copy()]\n",
    "    for _ in range(steps):\n",
    "        grad = gradient(pos[0], pos[1])\n",
    "        velocity = momentum * velocity + grad\n",
    "        pos = pos - lr * velocity\n",
    "        history.append(pos.copy())\n",
    "    return np.array(history)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (momentum, title) in zip(axes, [(0.0, 'SGD (no momentum)'), (0.9, 'SGD with Momentum (β=0.9)')]):\n",
    "    ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "    path = simulate_sgd([4, 4], lr=0.3, steps=30, momentum=momentum)\n",
    "    ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=4, linewidth=1.5, label='Optimization path')\n",
    "    ax.plot(path[0, 0], path[0, 1], 'go', markersize=10, label='Start')\n",
    "    ax.plot(0, 0, 'r*', markersize=15, label='Optimum')\n",
    "    ax.set_xlabel('Weight 1')\n",
    "    ax.set_ylabel('Weight 2')\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RMSprop: Adaptive Learning Rates\n",
    "\n",
    "**Key idea:** Adapt the learning rate for each parameter based on recent gradient magnitudes.\n",
    "\n",
    "$$s_t = \\beta s_{t-1} + (1-\\beta)(\\nabla \\mathcal{L})^2$$\n",
    "$$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} \\nabla \\mathcal{L}$$\n",
    "\n",
    "**Effect:** \n",
    "- Large gradients → smaller effective learning rate\n",
    "- Small gradients → larger effective learning rate\n",
    "\n",
    "This naturally handles the ravine problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Adam: The Best of Both Worlds\n",
    "\n",
    "**Adam** (Adaptive Moment Estimation) combines:\n",
    "- Momentum (first moment)\n",
    "- RMSprop (second moment)\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla \\mathcal{L}$$ (momentum)\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla \\mathcal{L})^2$$ (RMSprop)\n",
    "\n",
    "With bias correction:\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "\n",
    "Update:\n",
    "$$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "**Default hyperparameters:** $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-7}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Adam is the Default\n",
    "\n",
    "| Property | SGD | SGD+Momentum | RMSprop | Adam |\n",
    "|----------|-----|--------------|---------|------|\n",
    "| Adaptive LR per param | ✗ | ✗ | ✓ | ✓ |\n",
    "| Momentum | ✗ | ✓ | ✗ | ✓ |\n",
    "| Bias correction | N/A | N/A | ✗ | ✓ |\n",
    "| Works out-of-the-box | ✗ | ~ | ~ | ✓ |\n",
    "\n",
    "**Adam is robust:** It usually works well with default settings, making it the go-to choice for most problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of optimizers\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "ax.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\n",
    "\n",
    "# Simulate different optimizers (simplified)\n",
    "optimizers = {\n",
    "    'SGD': {'momentum': 0.0, 'lr': 0.15, 'color': 'red'},\n",
    "    'SGD+Momentum': {'momentum': 0.9, 'lr': 0.15, 'color': 'blue'},\n",
    "    'Adam (simulated)': {'momentum': 0.9, 'lr': 0.5, 'color': 'green'},  # Simplified\n",
    "}\n",
    "\n",
    "for name, params in optimizers.items():\n",
    "    path = simulate_sgd([4, 4], lr=params['lr'], steps=25, momentum=params['momentum'])\n",
    "    ax.plot(path[:, 0], path[:, 1], 'o-', color=params['color'], \n",
    "            markersize=3, linewidth=1.5, label=name, alpha=0.8)\n",
    "\n",
    "ax.plot(4, 4, 'ko', markersize=12, label='Start')\n",
    "ax.plot(0, 0, 'k*', markersize=15, label='Optimum')\n",
    "ax.set_xlabel('Weight 1', fontsize=12)\n",
    "ax.set_ylabel('Weight 2', fontsize=12)\n",
    "ax.set_title('Optimizer Comparison on a Ravine Loss Surface', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(-2, 5)\n",
    "ax.set_ylim(-2, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learning Rate Schedules\n",
    "\n",
    "A fixed learning rate isn't always optimal. **Schedules** adjust the learning rate during training.\n",
    "\n",
    "### Common Schedules\n",
    "\n",
    "| Schedule | Formula | Use Case |\n",
    "|----------|---------|----------|\n",
    "| **Step decay** | Reduce by factor every N epochs | Simple, predictable |\n",
    "| **Exponential** | $\\eta_t = \\eta_0 \\cdot e^{-kt}$ | Smooth decay |\n",
    "| **Cosine annealing** | Follows cosine curve | State-of-the-art results |\n",
    "| **Reduce on plateau** | Reduce when val_loss stalls | Adaptive, practical |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate schedules\n",
    "epochs = np.arange(100)\n",
    "initial_lr = 0.01\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Step decay\n",
    "step_lr = initial_lr * (0.5 ** (epochs // 30))\n",
    "axes[0, 0].plot(epochs, step_lr, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Step Decay\\n(Halve every 30 epochs)')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Learning Rate')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Exponential decay\n",
    "exp_lr = initial_lr * np.exp(-0.03 * epochs)\n",
    "axes[0, 1].plot(epochs, exp_lr, 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('Exponential Decay')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Learning Rate')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cosine annealing\n",
    "cosine_lr = initial_lr * 0.5 * (1 + np.cos(np.pi * epochs / 100))\n",
    "axes[1, 0].plot(epochs, cosine_lr, 'r-', linewidth=2)\n",
    "axes[1, 0].set_title('Cosine Annealing')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Warmup + decay\n",
    "warmup_epochs = 10\n",
    "warmup_lr = np.where(epochs < warmup_epochs, \n",
    "                     initial_lr * epochs / warmup_epochs,\n",
    "                     initial_lr * np.exp(-0.03 * (epochs - warmup_epochs)))\n",
    "axes[1, 1].plot(epochs, warmup_lr, 'm-', linewidth=2)\n",
    "axes[1, 1].set_title('Warmup + Exponential Decay')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].axvline(x=warmup_epochs, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing an Optimizer: Practical Guide\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   OPTIMIZER DECISION TREE                   │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Start with Adam (lr=0.001)                                │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  Does it converge?                                         │\n",
    "│    │         │                                              │\n",
    "│   YES       NO                                              │\n",
    "│    │         │                                              │\n",
    "│    ▼         ▼                                              │\n",
    "│  Done!    Try lower lr (0.0001)                            │\n",
    "│              │                                              │\n",
    "│              ▼                                              │\n",
    "│         Still no? Try SGD + Momentum                       │\n",
    "│         (better generalization sometimes)                   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|----------------|\n",
    "| **Starting out** | Adam, lr=0.001 |\n",
    "| **Computer vision** | SGD+Momentum often better for final % |\n",
    "| **NLP / Transformers** | Adam or AdamW |\n",
    "| **Training is unstable** | Lower learning rate, add warmup |\n",
    "| **Stuck in plateau** | Try ReduceLROnPlateau callback |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## In Keras\n",
    "\n",
    "```python\n",
    "# Default Adam\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Adam with custom learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "# SGD with momentum\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    loss='categorical_crossentropy'\n",
    ")\n",
    "\n",
    "# Learning rate schedule with callback\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=5\n",
    ")\n",
    "model.fit(X, y, callbacks=[reduce_lr])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Start with Adam** — it works well out-of-the-box for most problems\n",
    "2. **Learning rate is the most important hyperparameter** — tune it first\n",
    "3. **Momentum helps** with noisy gradients and ravine-shaped loss surfaces\n",
    "4. **Adaptive optimizers** (RMSprop, Adam) adjust learning rates per-parameter\n",
    "5. **Learning rate schedules** can improve final performance\n",
    "6. **SGD+Momentum** can generalize better for computer vision (but harder to tune)\n",
    "\n",
    "**Next:** Apply these concepts in the labs. When training isn't working, come back here to diagnose optimizer issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

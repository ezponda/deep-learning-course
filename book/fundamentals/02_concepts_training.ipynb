{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts: Training Neural Networks\n",
    "\n",
    "Understanding overfitting, regularization, and hyperparameters.\n",
    "\n",
    "This notebook covers the **theory** of training. You'll apply these concepts in [04_lab_regularization](04_lab_regularization.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Training Loop\n",
    "\n",
    "Training a neural network is an iterative process:\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                      TRAINING LOOP                           │\n",
    "│                                                              │\n",
    "│   for each epoch:                                            │\n",
    "│       for each batch:                                        │\n",
    "│           1. Forward pass  → compute predictions             │\n",
    "│           2. Compute loss  → measure error                   │\n",
    "│           3. Backward pass → compute gradients               │\n",
    "│           4. Update weights → gradient descent               │\n",
    "│                                                              │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Epoch:** One complete pass through the entire training dataset  \n",
    "**Batch:** A subset of training samples processed together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overfitting vs Underfitting\n",
    "\n",
    "The fundamental challenge in machine learning: **generalization**.\n",
    "\n",
    "| Condition | Training Loss | Validation Loss | What's happening |\n",
    "|-----------|--------------|-----------------|------------------|\n",
    "| **Underfitting** | High | High | Model too simple |\n",
    "| **Good fit** | Low | Low | Model generalizes well |\n",
    "| **Overfitting** | Very low | High | Model memorized training data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = np.arange(1, 51)\n",
    "\n",
    "# Good fit\n",
    "train_loss_good = 1.0 * np.exp(-0.1 * epochs) + 0.1\n",
    "val_loss_good = 1.0 * np.exp(-0.08 * epochs) + 0.15\n",
    "\n",
    "# Overfitting\n",
    "train_loss_overfit = 1.0 * np.exp(-0.15 * epochs) + 0.02\n",
    "val_loss_overfit = 1.0 * np.exp(-0.08 * epochs) + 0.1 + 0.01 * epochs\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(epochs, train_loss_good, 'b-', label='Train', linewidth=2)\n",
    "axes[0].plot(epochs, val_loss_good, 'r-', label='Validation', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Good Fit', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs, train_loss_overfit, 'b-', label='Train', linewidth=2)\n",
    "axes[1].plot(epochs, val_loss_overfit, 'r-', label='Validation', linewidth=2)\n",
    "axes[1].axvline(x=20, color='green', linestyle='--', label='Stop here!', alpha=0.7)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Overfitting', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:** Watch the **validation loss**, not training loss!\n",
    "\n",
    "When validation loss starts increasing while training loss keeps decreasing → you're overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "Regularization **reduces overfitting** by constraining the model.\n",
    "\n",
    "### 1. L2 Regularization (Weight Decay)\n",
    "\n",
    "Add a penalty for large weights:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\sum_i w_i^2$$\n",
    "\n",
    "**Effect:** Keeps weights small, prevents the model from relying too heavily on any single feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dropout\n",
    "\n",
    "Randomly \"turn off\" neurons during training.\n",
    "\n",
    "```\n",
    "TRAINING (dropout = 0.5)\n",
    "─────────────────────────\n",
    "\n",
    "    ○───○───○        ○───✗───○        ○───○───✗\n",
    "    │   │   │        │       │        │   │    \n",
    "    ○───○───○   →    ✗───○───○   →    ○───✗───○\n",
    "    │   │   │        │   │   │        │       │\n",
    "    ○───○───○        ○───○───✗        ✗───○───○\n",
    "    \n",
    "    Full network     Batch 1          Batch 2\n",
    "                     (random drop)    (random drop)\n",
    "```\n",
    "\n",
    "**Effect:** Forces the network to learn redundant representations. No neuron can be relied upon exclusively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Early Stopping\n",
    "\n",
    "Stop training when validation loss stops improving.\n",
    "\n",
    "**How it works:**\n",
    "- Monitor validation loss each epoch\n",
    "- If it doesn't improve for `patience` epochs, stop\n",
    "- Restore the best weights\n",
    "\n",
    "This is the simplest and often most effective regularization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch Normalization\n",
    "\n",
    "Normalize the inputs to each layer:\n",
    "\n",
    "$$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$$\n",
    "\n",
    "Then scale and shift with learnable parameters:\n",
    "\n",
    "$$y = \\gamma \\hat{x} + \\beta$$\n",
    "\n",
    "**Effects:**\n",
    "- Faster training (can use higher learning rates)\n",
    "- Some regularization effect\n",
    "- Reduces sensitivity to weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Regularization Techniques\n",
    "\n",
    "| Technique | What it does | When to use |\n",
    "|-----------|--------------|-------------|\n",
    "| **L2 regularization** | Penalizes large weights | Always a good default |\n",
    "| **Dropout** | Randomly disables neurons | Dense layers, overfitting |\n",
    "| **Early stopping** | Stops training at best point | Always use! |\n",
    "| **Batch normalization** | Normalizes layer inputs | Deep networks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "**Hyperparameters** are settings you choose *before* training. They're not learned from data.\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "Controls how big the weight updates are.\n",
    "\n",
    "$$w_{\\text{new}} = w_{\\text{old}} - \\eta \\cdot \\nabla \\mathcal{L}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training(lr, n_steps=50):\n",
    "    w = 5.0  # Start far from optimal (w=0)\n",
    "    history = [w]\n",
    "    for _ in range(n_steps):\n",
    "        gradient = 2 * w  # Gradient of w^2\n",
    "        w = w - lr * gradient\n",
    "        history.append(w)\n",
    "    return history\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "learning_rates = [0.01, 0.1, 0.6]\n",
    "titles = ['Too Small (slow)', 'Good', 'Too Large (unstable)']\n",
    "colors = ['orange', 'green', 'red']\n",
    "\n",
    "for ax, lr, title, color in zip(axes, learning_rates, titles, colors):\n",
    "    history = simulate_training(lr)\n",
    "    ax.plot(history, linewidth=2, color=color)\n",
    "    ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Optimal')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Weight (w)')\n",
    "    ax.set_title(f'{title}\\nLR = {lr}', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-6, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rule of thumb:** Start with `0.001` and adjust based on training curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "Number of samples processed before updating weights.\n",
    "\n",
    "| Batch Size | Pros | Cons |\n",
    "|------------|------|------|\n",
    "| **Small** (16-32) | Better generalization, noisier gradients | Slower, less stable |\n",
    "| **Large** (128-512) | Faster, more stable | May generalize worse |\n",
    "\n",
    "**Common choices:** 32, 64, 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Epochs\n",
    "\n",
    "How many times to iterate over the training data.\n",
    "\n",
    "**Best practice:** Use **early stopping** instead of a fixed number. Set a high max (e.g., 100) and let early stopping decide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture\n",
    "\n",
    "| Hyperparameter | Effect of increasing |\n",
    "|----------------|----------------------|\n",
    "| **Number of layers** | More complex patterns, risk of overfitting |\n",
    "| **Neurons per layer** | Higher capacity, more parameters |\n",
    "| **Dropout rate** | More regularization, may underfit |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train / Validation / Test Split\n",
    "\n",
    "Always split your data into three sets:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        ALL DATA                             │\n",
    "├─────────────────────────────┬───────────────┬───────────────┤\n",
    "│         TRAINING            │  VALIDATION   │     TEST      │\n",
    "│           (70%)             │    (15%)      │    (15%)      │\n",
    "│                             │               │               │\n",
    "│    Used to update           │  Used to      │  Final        │\n",
    "│    weights                  │  tune         │  evaluation   │\n",
    "│                             │  hyperparams  │  (touch once) │\n",
    "└─────────────────────────────┴───────────────┴───────────────┘\n",
    "```\n",
    "\n",
    "**Important:** Never use the test set to make decisions during training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Watch validation loss** — it tells you if you're overfitting\n",
    "2. **Use early stopping** — simplest and most effective regularization\n",
    "3. **Start with small learning rate** — 0.001 is a good default\n",
    "4. **Dropout** helps with overfitting in dense layers\n",
    "5. **Batch normalization** speeds up training and adds regularization\n",
    "6. **Keep a test set** that you only touch at the very end\n",
    "\n",
    "**Next:** Apply these techniques in [04_lab_regularization](04_lab_regularization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

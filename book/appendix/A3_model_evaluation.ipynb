{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/deep-learning-course/blob/main/book/appendix/A3_model_evaluation.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Model Evaluation & Debugging\n",
    "\n",
    "Beyond accuracy: metrics, visualization, and troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why Accuracy Isn't Enough\n",
    "\n",
    "Consider a fraud detection model with 99% accuracy. Sounds great, right?\n",
    "\n",
    "But if only 1% of transactions are fraudulent, a model that **always predicts \"not fraud\"** achieves 99% accuracy!\n",
    "\n",
    "**Accuracy hides important information** when classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Confusion Matrix\n",
    "\n",
    "A confusion matrix shows **all four types of predictions**:\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "                 Neg      Pos\n",
    "              ┌────────┬────────┐\n",
    "    Actual    │   TN   │   FP   │  Negative\n",
    "              ├────────┼────────┤\n",
    "              │   FN   │   TP   │  Positive\n",
    "              └────────┴────────┘\n",
    "\n",
    "TN = True Negative  (correct rejection)\n",
    "FP = False Positive (false alarm, Type I error)\n",
    "FN = False Negative (miss, Type II error)\n",
    "TP = True Positive  (correct detection)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Binary classification results\n",
    "y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Predicted Neg', 'Predicted Pos'],\n",
    "            yticklabels=['Actual Neg', 'Actual Pos'])\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"True Negatives: {tn}, False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}, True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Precision, Recall, and F1-Score\n",
    "\n",
    "### Precision (Positive Predictive Value)\n",
    "\n",
    "\"Of all predicted positives, how many are actually positive?\"\n",
    "\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**High precision = few false alarms**\n",
    "\n",
    "### Recall (Sensitivity, True Positive Rate)\n",
    "\n",
    "\"Of all actual positives, how many did we catch?\"\n",
    "\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**High recall = few misses**\n",
    "\n",
    "### F1-Score (Harmonic Mean)\n",
    "\n",
    "Balances precision and recall:\n",
    "\n",
    "$$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision vs Recall: The Trade-off\n",
    "\n",
    "| Scenario | Prioritize | Why |\n",
    "|----------|------------|-----|\n",
    "| **Spam filter** | Precision | Don't want important emails in spam |\n",
    "| **Cancer screening** | Recall | Don't want to miss any cases |\n",
    "| **Fraud detection** | Depends on cost | Balance false alarms vs. missed fraud |\n",
    "| **Search engine** | Precision | Users want relevant results first |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the precision-recall trade-off\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# High threshold (high precision, low recall)\n",
    "axes[0].bar(['Precision', 'Recall', 'F1'], [0.95, 0.40, 0.56], color=['green', 'orange', 'blue'])\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_title('High Threshold\\n(Conservative)', fontsize=12)\n",
    "axes[0].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Balanced threshold\n",
    "axes[1].bar(['Precision', 'Recall', 'F1'], [0.75, 0.75, 0.75], color=['green', 'orange', 'blue'])\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_title('Balanced Threshold\\n(Default)', fontsize=12)\n",
    "axes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Low threshold (low precision, high recall)\n",
    "axes[2].bar(['Precision', 'Recall', 'F1'], [0.45, 0.95, 0.61], color=['green', 'orange', 'blue'])\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_title('Low Threshold\\n(Aggressive)', fontsize=12)\n",
    "axes[2].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ROC Curve and AUC\n",
    "\n",
    "The **ROC curve** (Receiver Operating Characteristic) shows the trade-off between:\n",
    "- **True Positive Rate** (Recall)\n",
    "- **False Positive Rate** (1 - Specificity)\n",
    "\n",
    "**AUC** (Area Under Curve) summarizes performance: \n",
    "- AUC = 1.0 → Perfect\n",
    "- AUC = 0.5 → Random guessing\n",
    "- AUC < 0.5 → Worse than random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated probability scores\n",
    "np.random.seed(42)\n",
    "y_true_proba = np.array([0]*50 + [1]*50)\n",
    "# Good model: positive class has higher scores\n",
    "y_scores = np.concatenate([\n",
    "    np.random.beta(2, 5, 50),  # Negative class: lower scores\n",
    "    np.random.beta(5, 2, 50)   # Positive class: higher scores\n",
    "])\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true_proba, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random classifier (AUC = 0.50)')\n",
    "plt.fill_between(fpr, tpr, alpha=0.3)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve', fontsize=14)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-class Metrics\n",
    "\n",
    "For multi-class problems, metrics are computed per-class then averaged:\n",
    "\n",
    "| Averaging | Method |\n",
    "|-----------|--------|\n",
    "| **Macro** | Average metrics across classes (treats all classes equally) |\n",
    "| **Weighted** | Average weighted by class frequency |\n",
    "| **Micro** | Compute globally (total TP, FP, FN) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-class example\n",
    "y_true_multi = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 0, 1, 2])\n",
    "y_pred_multi = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 0, 0, 2, 0, 1, 1])\n",
    "\n",
    "cm_multi = confusion_matrix(y_true_multi, y_pred_multi)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm_multi, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Class 0', 'Class 1', 'Class 2'],\n",
    "            yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.title('Multi-class Confusion Matrix', fontsize=14)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true_multi, y_pred_multi, \n",
    "                            target_names=['Class 0', 'Class 1', 'Class 2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Training Problems & Solutions\n",
    "\n",
    "### Problem: Loss Not Decreasing\n",
    "\n",
    "| Symptom | Likely Cause | Solution |\n",
    "|---------|--------------|----------|\n",
    "| Loss stays high | Learning rate too low | Increase LR |\n",
    "| Loss oscillates wildly | Learning rate too high | Decrease LR |\n",
    "| Loss is NaN | Exploding gradients | Lower LR, add gradient clipping |\n",
    "| Loss stuck at constant | Dead neurons (ReLU) | Use LeakyReLU, check initialization |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learning rate problems\n",
    "epochs = np.arange(50)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Too low\n",
    "loss_low = 2.5 - 0.02 * epochs + np.random.normal(0, 0.05, 50)\n",
    "axes[0].plot(epochs, loss_low, 'b-', linewidth=2)\n",
    "axes[0].set_title('Learning Rate Too Low\\n(Barely decreasing)', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_ylim(0, 3)\n",
    "\n",
    "# Good\n",
    "loss_good = 2.5 * np.exp(-0.1 * epochs) + 0.1 + np.random.normal(0, 0.03, 50)\n",
    "axes[1].plot(epochs, loss_good, 'g-', linewidth=2)\n",
    "axes[1].set_title('Good Learning Rate\\n(Smooth decrease)', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_ylim(0, 3)\n",
    "\n",
    "# Too high\n",
    "loss_high = 1.5 + 0.8 * np.sin(epochs * 0.5) + np.random.normal(0, 0.2, 50)\n",
    "axes[2].plot(epochs, loss_high, 'r-', linewidth=2)\n",
    "axes[2].set_title('Learning Rate Too High\\n(Oscillating)', fontsize=12)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Loss')\n",
    "axes[2].set_ylim(0, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Overfitting\n",
    "\n",
    "| Symptom | Solution |\n",
    "|---------|----------|\n",
    "| Val loss increases while train loss decreases | Early stopping |\n",
    "| Large gap between train/val accuracy | Add dropout, regularization |\n",
    "| Model memorizes training data | Get more data, augmentation |\n",
    "| Perfect training accuracy | Reduce model complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Underfitting\n",
    "\n",
    "| Symptom | Solution |\n",
    "|---------|----------|\n",
    "| Both train and val loss stay high | Increase model capacity |\n",
    "| Model too simple for data | Add more layers/neurons |\n",
    "| Training stops too early | Increase epochs, tune LR |\n",
    "| Poor feature representation | Better preprocessing, feature engineering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Debugging Checklist\n",
    "\n",
    "```\n",
    "□ Data\n",
    "  □ Are inputs normalized? (mean ~0, std ~1)\n",
    "  □ Are labels correct? (spot-check a few examples)\n",
    "  □ Is there data leakage? (val/test data in training)\n",
    "  □ Are classes balanced? (if not, use class weights)\n",
    "\n",
    "□ Model\n",
    "  □ Can it overfit a tiny dataset? (sanity check)\n",
    "  □ Are activation functions appropriate?\n",
    "  □ Is output layer correct for the task?\n",
    "  \n",
    "□ Training\n",
    "  □ Is loss decreasing? (if not, check LR)\n",
    "  □ Is validation loss tracked? (use callbacks)\n",
    "  □ Are gradients flowing? (no NaN, not all zeros)\n",
    "  \n",
    "□ Evaluation\n",
    "  □ Using appropriate metrics? (not just accuracy)\n",
    "  □ Test set never seen during training?\n",
    "  □ Results reproducible? (set random seeds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Keras Callbacks for Monitoring\n",
    "\n",
    "```python\n",
    "from tensorflow import keras\n",
    "\n",
    "callbacks = [\n",
    "    # Stop when val_loss stops improving\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    \n",
    "    # Reduce LR when stuck\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='./logs'\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=callbacks,\n",
    "          epochs=100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Don't rely on accuracy alone** — use precision, recall, F1 for imbalanced data\n",
    "2. **Confusion matrix reveals errors** — see exactly what's being confused\n",
    "3. **ROC-AUC for probability outputs** — threshold-independent performance measure\n",
    "4. **Watch both train and val loss** — the gap tells you about overfitting\n",
    "5. **Use the debugging checklist** — systematic approach to fixing problems\n",
    "6. **Keras callbacks automate monitoring** — EarlyStopping, ReduceLROnPlateau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
